version: '3.8'

services:
  tensorflow:
    build: docker_services/dev/
    container_name: dev
    ports:
      - "8000:8000"  # Host port 8000 mapped to container port 8000
      - "8001:8001"  # Host port 8001 mapped to container port 8001
    volumes:
      - .:/workspace  # Mount the current directory into the container
    stdin_open: true  # Keep STDIN open for interactive sessions
    tty: true         # Allocate a pseudo-TTY for the container
    command: >
      jupyter lab --allow-root --port=8000 --no-browser --ip=0.0.0.0 --NotebookApp.token='' --NotebookApp.password='' --ServerApp.port_retries=0

  preprocessing:
    build: docker_services/preprocessing/
    container_name: preprocessing
    volumes:
      - .:/workspace # Mount the current directory into the container
    stdin_open: true  # Keep STDIN open for interactive sessions
    tty: true         # Allocate a pseudo-TTY for the container

  modeling:
    build: docker_services/modeling/
    container_name: modeling
    ports:
      - "8080:8080"  # Exposing port 8080 for MLflow UI
    volumes:
      - .:/workspace # Mount the current directory into the container
    stdin_open: true  # Keep STDIN open for interactive sessions
    tty: true         # Allocate a pseudo-TTY for the container
    command: >
      mlflow server --host 0.0.0.0 --port 8080 --backend-store-uri sqlite:///mlflow.db --default-artifact-root /workspace/mlruns

  deployment:
    build: docker_services/deployment/
    container_name: deployment
    ports:
      - "8888:8888"
    volumes:
      - .:/workspace
    stdin_open: true  # Keep STDIN open for interactive sessions
    tty: true         # Allocate a pseudo-TTY for the container
    # command: >
    #   uvicorn app:app --host 0.0.0.0 --port 8888 --reload



